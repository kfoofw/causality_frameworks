{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Probability of Treatment Weighting (IPTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivating Example\n",
    "- Supposed that there is a single binary confounder X\n",
    "\n",
    "Suppose that P(A=1|X=1) = 0.1\n",
    "- Among people with X = 1, only 10% will receive the treatment\n",
    "- The value of the propensity score for people with X = 1 is 0.1\n",
    "\n",
    "Supposed that P(A=1|X=0) = 0.8\n",
    "- Among people with X = 0, 80% will receive treatment\n",
    "- The value of the propensity score for people with X = 0 is 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/iptw_start_1.png\" >\n",
    "\n",
    "\n",
    "### Treated Group (X = 1)\n",
    "\n",
    "<img src=\"./img/iptw_start_2.png\" >\n",
    "\n",
    "__Propensity score matching:__\n",
    "- Randomly choose 1 out of 9 control subjects to match with the only 1 treated subject.\n",
    "- The control subject match is supposed to represent 9 others, but only uses information from itself.\n",
    "- Thus, it can be seen as some form of discarding of information\n",
    "<img src=\"./img/iptw_start_3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IPTW:__   \n",
    "- Rather than match, we can use all of the data but down-weight some and up-weight others.\n",
    "    - You do not discard data in that sense from unmatched subjects.\n",
    "- This is accomplished by weighting by the inverse of the probability of treatment received.\n",
    "    - For treated subjects, weight by the inverse of P(A=1|X)\n",
    "    - For control subjects, weight by the inverse of P(A=0|X)\n",
    "\n",
    "<img src=\"./img/iptw_start_4.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Group (X = 0)\n",
    "\n",
    "Propensity Score Matching for X = 0:\n",
    "<img src=\"./img/iptw_start_5.png\" >\n",
    "\n",
    "IPTW for X = 0:\n",
    "<img src=\"./img/iptw_start_6.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survey Sampling\n",
    "- In surveys, it is common to oversample some groups relative to the population\n",
    "    - Oversample a minority group\n",
    "    - Oversample older adults\n",
    "    - Oversample obese individuals\n",
    "- To estimate the popualtion mean, we can weight the data to account for the oversample\n",
    "    - Horvitz-Thompson estimator\n",
    "    \n",
    "This relates to observational studies.\n",
    "\n",
    "In an observational study, certain groups are oversampled relative to the hypothetical sample from a randomized trial\n",
    "- There is confounding in the original population\n",
    "- IPTW creates a pseudo-population where treatment assignment no longer depends on X\n",
    "    - No confounding in the pseudo population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose P(A=1|X) = 0.9\n",
    "- Apply weighting based on inverse probability weighing\n",
    "- Create a pseudo population as if it was from a randomised control trial\n",
    "<img src=\"./img/iptw_observational_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the original population, some people were more likely to get treated than others based on their Xs.\n",
    "- In the pseudo-population, everyone is equally likely to be treated regardless of their X values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the assumption of exchangeability and positivity, we can estimate the __expected value of a potential outcome with treatment $E(Y^1)$__ as:\n",
    "\n",
    "<img src=\"./img/iptw_formula_estimator.png\" >\n",
    "\n",
    "where $\\pi_i = P(A=1|X_i)$ is the propensity score.\n",
    "\n",
    "Note that the formula showcases how positivity assumption comes into play. If the propensity for a given X is 0, one of the values in the formula will be dividing by 0 which will result in a NaN number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Structural Models\n",
    "- A marginal structural model (MSM) is a model for the mean of the potential outcomes\n",
    "- __Marginal__: model that is not conditional on the confounders. we want to find the causal effect on the population, and not a subpopulation.\n",
    "- __Structural__: Model for potential outcomes, not observed outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Linear MSM__\n",
    "\n",
    "$E(Y^a) = \\phi_0 + \\phi_1 a,\\ where\\ a=0,1$\n",
    "\n",
    "- $E(Y^0) = \\phi_0$\n",
    "- $E(Y^1) = \\phi_0 + \\phi_1$\n",
    "- Thus, $\\phi_1$ is the average causal effect $E(Y^1) - E(Y^0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Logistic MSM (for binary outcome)__ \n",
    "\n",
    "$logit\\{ E(Y^a) \\} = \\phi_0 + \\phi_1 a,\\ where \\ a = 0,1$\n",
    "\n",
    "So $exp(\\phi_1)$ is the causal odds ratio\n",
    "\n",
    "<img src=\"./img/msm_logistic_odds_ratio.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __MSM with Effect Modification__\n",
    "- MSMs can also include effect modifiers\n",
    "- Suppose V is a variable that modifies the effect of A\n",
    "- A linear MSM with effect modification:\n",
    "    - This captures heterogeneity of treatment effect where the treatment effect might vary across different subpopulations\n",
    "\n",
    "$E(Y^a|V) = \\phi_0 + \\phi_1 a + \\phi_3 V + \\phi_4 a V, \\ where\\ a = 0,1$\n",
    "\n",
    "Thus, the causal effect for a subpopulation with V is:   \n",
    "\n",
    "$E(Y^1|V) - E(Y^0|V) = \\phi_1 + \\phi_4 V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __General MSM__\n",
    "\n",
    "MSM models can be expanded to a general formula that is similar to that of generalised linear models with different link functions:\n",
    "\n",
    "$g\\{E(Y^a|V)\\} = h(a,V;\\phi)\\ where $\n",
    "\n",
    "- g() is a link function\n",
    "- h() is a function specifying parametric form of a and V (typically additive, linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation in generalised linear regression models\n",
    "- Estimation of parameters from a GLM:\n",
    "\n",
    "$E(Y_i | X_i) = \\mu_i = g^{-1}(X^T_i \\beta)$\n",
    "\n",
    "- Estimation involves solving \n",
    "\n",
    "$\\sum^{n}_{i=1} \\frac{\\mathrm{d}\\mu_i^T}{\\mathrm{d} \\beta} V_i^{-1} \\{Y_i - \\mu_i(\\beta)\\} = 0\\ for\\ \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation in MSMs\n",
    "- Marginal structural models look a lot like generalised linear models.\n",
    "    - For example, $E(Y_i^a) = g^{-1} (\\phi_0 + \\phi_1 a)$\n",
    "        - This involves \"setting of treatment\" and is applied to the whole population for potential outcome\n",
    "- This model is __not equivalent (due to confounding)__ to the regression model\n",
    "    - $E(Y_i|A_i) = g^{-1}(\\phi_0 + \\phi_1 A_i)$\n",
    "        - This involves \"conditioning of treatment\" and is applied to only a subpopulation with possible confounding \n",
    "\n",
    "__Note the distinction between \"Setting\" and \"Conditioning\":__\n",
    "- Setting is for MSMs\n",
    "- Conditioning is for GLMs.\n",
    "    - If you had a randomised trial, the parameters for the GLM regression model should represent a causal effect because there is no confounding.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that the __pseudo-population__ (obtained from IPTW) is free from confounding (assuming ignorability and positivity).\n",
    "- By using the inverse propensity weights, we can apply those weights to the observed data and create a pseudo population that does not have confounding. \n",
    "- We can therefore estimate MSM parameters by solving estimating equations for the observed data of the __pseudo-population__.\n",
    "  \n",
    "  $\\sum^{n}_{i=1} \\frac{\\mathrm{d}\\mu_i^T}{\\mathrm{d} \\phi} V_i^{-1} W_i \\{Y_i - \\mu_i(\\phi)\\} = 0\\ for\\ \\beta,\\ where\\ W_i = \\frac{1}{A_iP(A=1|X_i)\\ +\\ (1-A_i)P(A=0|X_i)}$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. Estimate propensity score (using a logistic regression model etc)\n",
    "2. Create weights\n",
    "    - 1 divided by propensity score for treated subjects\n",
    "    - 1 divided by 1 minus the propensity score for control subjects\n",
    "3. Specify the MSM of interest\n",
    "4. Use software to fit a weighted generalised linear model\n",
    "5. Use asymptotic (sandwich) variance estimator (or bootstrapping)\n",
    "    - This accounts for the fact that pseudo-population might be larger than sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Balance after weighting__\n",
    "- Covariate balance can be checked on the weighted sample using standardized differences.\n",
    "    - In a Table 1\n",
    "    - In a plot\n",
    "\n",
    "Recall the __standardised difference__: \n",
    "- It is the difference in means between groups, divided by the (pooled) standard deviation:\n",
    "$SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}$\n",
    "- It was used for covariate balancing as shown in previously mentioned matching techniques\n",
    "\n",
    "__SMD can be used on IPTW methods__\n",
    "- Same idea, except on __weighted means and weighted variances__\n",
    "    - Stratify on treatment group\n",
    "        - Find weighted mean and weighted variance for each group\n",
    "        - This can be done directly or with software tools that were developed for surveys (e.g. svydesign in R)\n",
    "        - Take difference in weighted means and divide by an estimate of the pooled (weighted) standard deviation\n",
    "        \n",
    "__Example using RHC data with Weighted Pseudo-RCT-population__\n",
    "\n",
    "<img src=\"./img/iptw_smd.png\" >\n",
    "\n",
    "<img src=\"./img/iptw_smd_plot.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is covariate imbalance after weighting is already performed:\n",
    "- Propensity score model should be refined with the following considerations:\n",
    "    - Interactions\n",
    "    - Non linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Weights\n",
    "\n",
    "Issues with weights:\n",
    "- Larger weights lead to noisier estimates of causal effects\n",
    "- Example:\n",
    "    - Suppose in a treatment group, 1 person has a weight of 10,000\n",
    "    - If the outcome is binary (whether they have the event or not) could have a big impact on the parameter estimate\n",
    "    - Essentially, if one person's outcome data can greatly affect the parameter estimate, then the __standard error will be large__.\n",
    "    \n",
    "Further intuition with __Bootstrapping__:\n",
    "1. Randomly sample with replacement from the original sample\n",
    "2. Estimate parameters\n",
    "3. Repeat steps 1 and 2 many times.\n",
    "4. The standard deviation of the bootstrap estimates is an estimate of the standard error.\n",
    "\n",
    "Implications of Bootstrap results\n",
    "- Someone with a very large weight will be included in some bootstrap samples (with possibility of repetition), but not others.\n",
    "- Whether or not they are included will have a relatively large impact on the parameter estimates\n",
    "- Thus, a lot of the variability of the estimator would be due to this one subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship with Positivity Assumption:\n",
    "- An extremely large weight means that the proability of that treatment was very small.\n",
    "    - Thus, large weights indicate near violations of the positivity assumption\n",
    "        - People with certain values of the covariates are very unlikely to get one of the treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weights Distribution Plot for Diagnosis:__\n",
    "\n",
    "<img src=\"./img/iptw_weights_distribution.png\" >\n",
    "\n",
    "__Summary statistics of weights:__\n",
    "\n",
    "<img src=\"./img/iptw_weights_table.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Weights\n",
    "\n",
    "A good first step is to look into why the weights are large. Identify the subjects who ahve large weights:\n",
    "- What is unusual about them?\n",
    "- Is there a problem with their data?\n",
    "- Is there a problem with the propensity score model?\n",
    "\n",
    "Very large weights: investigative step\n",
    "- Example: suppose there is one confounder and you fit a logistic regression propensity score model:\n",
    "$logit(\\pi_i) = \\beta_0 + \\beta_1 X_i$\n",
    "\n",
    "<img src=\"./img/iptw_large_weights_1.png\" >\n",
    "\n",
    "<img src=\"./img/iptw_large_weights_2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For data points in the huge cluster, the propensity model of the logit is probably accurate within that region.\n",
    "- For the outlier, it is too far out to the extreme with no other data points. Thus, the extrapolation of the logit curve might not actually be accurate.\n",
    "    - Note the plateau of the curve, which was extrapolated without good support in the extreme end.\n",
    "    - For example, an alternative red curve for the logit curve could be here:\n",
    "    <img src=\"./img/iptw_large_weights_3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming the tails\n",
    "- Large weights occur at observations in the __tails of the propensity score distribution__\n",
    "- Trimming the tails can eliminate some of the extreme weights\n",
    "    - This means __removing subjects__ who have __extreme values__ of the propensity score\n",
    "        - A common trimming straegy:\n",
    "            - Remove treated subjects whose propensity scores are above the 98th percentile from the distribution among __controls__\n",
    "            - Remove control subjects whose propensity scores are below the 2nd percentile from the distribution of __treated subjects__\n",
    "- __Reminder: trimming the tails changes the population!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Truncation\n",
    "\n",
    "Truncating the large weights is another option. Steps:\n",
    "- Determine a maximum allowable weight\n",
    "    - Can be a specific value\n",
    "    - Can be based on a percentile\n",
    "- If a weight is greater than the mx allowable, set it to the maximum allowable value\n",
    "    - If your limit is 100 and someone has a weight of 2000, set theri weight to 100 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not to truncate weights involves a bias-variance trade-off:\n",
    "- __Truncation__: bias, but smaller variance\n",
    "- __No truncation__: unbiased, larger variance\n",
    "\n",
    "Truncating extremely large weights can result in estimators with lower mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubly Robust Estimation (aka Augmented Inverse Probabiliyt of Treatment Weighting)\n",
    "\n",
    "__Method 1: Potential Outcome Estimation with IPTW__\n",
    "- We can estimate $E(Y^1)$ using IPTW  \n",
    "    $\\frac{1}{n}\\sum{n}{i=1} \\frac{A_i Y_i}{\\pi_i(X_i)},\\ where\\ \\pi_i(X_i)\\ are\\ the\\ inverse\\ weights$\n",
    "    \n",
    "- If the propensity score is correctly specified, this estimator is unbiased.\n",
    "\n",
    "__Method 2: Regression-based estimation__\n",
    "- Alternatively, we can estimate $E(Y^1)$ by specifying an outcome regression model $m_1(X) = E(Y|A=1,X)$ and then averaging over the distribution of X.\n",
    "    - NOTE that $m_1(X)$ is the regression model for outcome with Treatment A = 1\n",
    "- If outcome model is correctly specified, then this estimator is unbiased.\n",
    "\n",
    "<img src=\"./img/doubly_robust_estimate_regression_1.png\" >\n",
    "\n",
    "Key Breakdown:\n",
    "- For treated subjects, we will definitely use their observed outcome Y\n",
    "- For non-treated subjects, we will __predict their value of outcome Y given their X IF they had received treatment (A = 1)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Doubly Robust Estimator__ is an estimator that is unbiased if __either the pronesity score model OR the outome regression model are correctly specified__. An example of this is:\n",
    "\n",
    "<img src=\"./img/doubly_robust_estimate_formula_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doubly robust estimator is made of two components:\n",
    "- IPTW: Dependent on the propensity scoring model through the weights.\n",
    "- Augmentation: Dependent on the outcome regression model\n",
    "\n",
    "__If propensity score is correctly specified, but outcome model is not__:\n",
    "- Recall that $A_i$ is the treatment applied, while $\\pi_i(X_i)$ is the propensity score.\n",
    "- Expectation of $A_i$ is equal to the propensity score if propensity score model is correctly specified.\n",
    "- __Weight of the Augmentation part has expectation of 0 based on the numerator.__\n",
    "- This will leave us with only the IPTW part of the formula.\n",
    "\n",
    "<img src=\"./img/doubly_robust_estimate_breakdown_1.png\" >\n",
    "\n",
    "__If propensity score is wrong, but outcome model is correct__:\n",
    "- Outcome model is correct implies that the expectation of Y conditional on X should be equal to $m_1(X_i)$\n",
    "- The above formula can be re-written into the following:\n",
    "\n",
    "<img src=\"./img/doubly_robust_estimate_breakdown_2.png\" >\n",
    " \n",
    "- Given that the outcome model is correct, the numerator portion $Y_i - m_1(X_i)$ is expected to be 0.\n",
    "- This only leaves the 2nd portion of the revised formula with only $m_1(X_i)$ which is the expected value of $Y^1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubly robust estimators are also known as augmented IPTW (AIPTW) estimators.\n",
    "- Can use semi parametric theory to identify best estimators\n",
    "- In general, AIPTW estimators should be __more efficient__ than regular IPTW estimators.\n",
    "    - This means they have a smaller variance associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPTW Workflow in R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __RHC dataset:__\n",
    "    - Treatment: RHC or not\n",
    "    - Outcome: Death (yes/no)\n",
    "    - Confounders as Covariates X: x1, x2, x3\n",
    "- __Necessary packages__\n",
    "    - `library(tableone), library(ipw), library(sandwich), library(survey)`\n",
    "- __Propensity score__\n",
    "    - Create propensity score model with logistic regression\n",
    "        - `psmodel <- glm(treatment ~ x1 + x2 + x3, family = binomial(link = \"logit\"))`\n",
    "        - `ps <- predict(psmodel, type = \"response\")`\n",
    "    - Check plot of propensity score (pre-matching)\n",
    "        - <img src=\"./img/doubly_robust_estimate_workflow_1.png\" >\n",
    "- __IPTW (Part 1): Weights__\n",
    "    - Create weights and check balance (SMD) for the pseudo population\n",
    "        - `weight <- ifelse(treatment==1, 1/ps, 1/(1-ps))`\n",
    "        - `weighteddata <- svydesign(ids =~ 1, data = mydata, weights = ~ weights)`\n",
    "        - `weightedtable <- svyCreateTableOne(vars = xvars, strata = \"treatment\", data = weighteddata, test = FALSE)`\n",
    "- __IPTW (Part 2): MSMs__\n",
    "    - Fit the following MSM using IPTW\n",
    "        - $E(Y^a_i) = g^{-1}(\\phi_0 + \\phi_1 a)$\n",
    "            - A is treatment\n",
    "            - Y is death (yes, no)\n",
    "            - g() is the link function\n",
    "                - Log link for causal relative risk\n",
    "                - Identity link for causal risk difference\n",
    "    - Code (Relative Risk):\n",
    "        - Create MSM model through regression since it is a pseudo balanced (no confounding) population \n",
    "            - `glm.obj <- glm(died ~ treatment, weights = weight, family = binomial(link = log))`\n",
    "        - Check summary\n",
    "            - `betaiptw <- coef(glm.obj)`\n",
    "        - To properly account for weighting\n",
    "            - `SE <- srqt(diag(vcovHC(glm.obj, type = \"HC0\")))`\n",
    "        - Extract out point estimates and CI for relative risk\n",
    "            - `causalrr <- exp(betaiptw[2])`\n",
    "            - `lcl <- exp(betaiptw[2] - 1.96*SE[2])`\n",
    "            - `ucl <- exp(betaiptw[2] + 1.96*SE[2])`\n",
    "            - `c(lcl, causalrr, ucl)`\n",
    "    - Code (Causal Risk Difference):\n",
    "        - Create MSM model through regression since it is a pseudo balanced (no confounding) population \n",
    "            - `glm.obj <- glm(died ~ treatment, weights = weight, family = binomial(link = \"identity\"))`\n",
    "        - Check summary\n",
    "            - `betaiptw <- coef(glm.obj)`\n",
    "        - To properly account for weighting\n",
    "            - `SE <- srqt(diag(vcovHC(glm.obj, type = \"HC0\")))`\n",
    "        - Extract out point estimates and CI for causal risk\n",
    "            - `causalrd <- exp(betaiptw[2])`\n",
    "            - `lcl <- exp(betaiptw[2] - 1.96*SE[2])`\n",
    "            - `ucl <- exp(betaiptw[2] + 1.96*SE[2])`\n",
    "            - `c(lcl, causalrd, ucl)`\n",
    "        \n",
    "\n",
    "\n",
    "__Alternative method with IPW package__\n",
    "- __Propensity Scoring:__\n",
    "    - Create propensity score model with logistic regression\n",
    "        - `weightmodel <- ipwpoint(exposure = treatment, family = \"binomial\", link = \"logit\", denominator ~ x1 + x2 + x3, data = mydata)\n",
    "        - `summary(weightmodel$ipw.weights)`\n",
    "    - Check plot of propensity score (pre-matching)\n",
    "        - `ipwplot(weights = weightmodel$ipw.weights, logscale = FALSE, main =  \"weights\", xlim = c(0, 22))`\n",
    "            - <img src=\"./img/doubly_robust_estimate_workflow_2.png\" >\n",
    "- __Fit the MSM__\n",
    "    - Create MSM model through regression since it is a pseudo balanced (no confounding) population. svyglm function will do the robust sandwich estimator automatically\n",
    "        - `msm <- (svyglm(died ~ treatment, design = svydesign(~1, weights = ~wt, data = mydata)))`\n",
    "        - `coef(msm)`\n",
    "        - `confint(msm)`\n",
    "\n",
    "__Truncating weights:__\n",
    "- __Non-IPW package:__\n",
    "    -  `truncweight <- replace(weight, weight > 10, 10)`\n",
    "    - `glm.obj <- glm(died~treatment, weights = truncweight, family = binomial(link = \"identity\"))`\n",
    "- __IPW package:__\n",
    "    - Truncation using percentile on both ends (ie. 0.01 implies 1st and 99th percentiles)\n",
    "        - `weightmodel <- ipwpoint(exposure = treatment, family = \"binomial\", link = \"logit\", denominator = ~ x1 + x2 + x3, data = mydata, trunc = 0.01)`\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
